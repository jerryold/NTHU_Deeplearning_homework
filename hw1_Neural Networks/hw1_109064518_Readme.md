# hw1_neural network
* Load mnist-training data and test data(檔案從colab雲端讀取)
```
trainingfilenames = {'images' : '/content/drive/My Drive/Colab Notebooks/MNIST/train-images.idx3-ubyte' ,'labels' : '/content/drive/My Drive/Colab Notebooks/MNIST/train-labels.idx1-ubyte'}
testfilenames = {'images' : '/content/drive/My Drive/Colab Notebooks/MNIST/t10k-images.idx3-ubyte' ,'labels' : '/content/drive/My Drive/Colab Notebooks/MNIST/t10k-labels.idx1-ubyte'}

data_types = {
        0x08: ('ubyte', 'B', 1),  #0x08: unsigned byte
        0x09: ('byte', 'b', 1),   #0x09: signed byte
        0x0B: ('>i2', 'h', 2),   #0x0B: short (2 bytes) 
        0x0C: ('>i4', 'i', 4),   #0x0C: int (4 bytes)
        0x0D: ('>f4', 'f', 4),   #0x0D: float (4 bytes)
        0x0E: ('>f8', 'd', 8)}   #0x0E: double (8 bytes)


```


* importing some libraires required for creating our neural network.
```
from __future__ import print_function
import numpy as np ## For numerical python
import numpy as np ## For numerical python
import struct as st
import matplotlib.pyplot as plt
import time
import math
np.random.seed(42)
``` 

* 將original data 分隔成 validation data and training data(3:7)
```
def shuffle_split_data(image,label,flatten=False):#分割image和label的函式(共6萬筆) train:validation=7:3

    
    #noramalize x
    X_image=image.astype(float)/255.
    
    

    # we reserve the last 18000 training examples for validation
    X_train, X_validation = X_image[:-18000], X_image[-18000:] 
    y_train, y_validation = label[:-18000], label[-18000:]

    if flatten:
      X_train = X_train.reshape([X_train.shape[0], -1])
      X_validation = X_validation.reshape([X_validation.shape[0], -1])
      

    print("Image:")
    print(len(X_train),len(X_validation))
    print("label:")
    print(len(y_train),len(y_validation))
    return X_train,X_validation,y_train,y_validation
```
* create a main class layer which can do a forward pass .forward() and Backward pass .backward()
```
class Layer:
     def __init__(self):
         pass  

---
- Process input to get output:           output = layer.forward(input)
- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)


---
    def forward(self, input):
         return input       

---
Takes input data of shape [batch, input_units], returns output data [batch, output_units]


---

    def backward(self, input, grad_output):
             
        num_units = input.shape[1]
        d_layer_d_input = np.eye(num_units)
        return np.dot(grad_output, d_layer_d_input) # chain rule

---
* Performs a backpropagation step through the layer, with respect to the given input.   
* To compute loss gradients w.r.t input, we need to apply chain rule (backprop):
    
* d loss / d x  = (d loss / d layer) * (d layer / d x) 


```
* Nonlinearity ReLU layer
```
    lass ReLU(Layer):
    def __init__(self):
       
        pass
* ReLU layer simply applies elementwise rectified linear unit to all inputs
    
    def forward(self, input):
       
        relu_forward = np.maximum(0,input)
        return relu_forward
* Apply elementwise ReLU to [batch, input_units] matrix"
    def backward(self, input, grad_output):
        relu_grad = input > 0
        return grad_output*relu_grad
* Compute gradient of loss w.r.t. ReLU input
```
* Dense layer
-dense layer applies affine transformation. In a vectorized form, it can be described as:f(x)=W*X+b
1. X is an object-feature matrix of shape [batch_size, num_features],
2. W is a weight matrix [num_features, num_outputs]
3. b is a vector of num_outputs biases.
```
class Dense(Layer):
    def __init__(self, input_units, output_units, learning_rate=0.1):
        
* A dense layer is a layer which performs a learned affine transformation: f(x) = <W*x> + b
        
        self.learning_rate = learning_rate
        self.weights = np.random.normal(loc=0.0, 
                                        scale = np.sqrt(2/(input_units+output_units)), 
                                        size = (input_units,output_units))
        self.biases = np.zeros(output_units)
        
    def forward(self,input):
        
* Perform an affine transformation:
* f(x) = <W*x> + b        
* input shape: [batch, input_units]
* output shape: [batch, output units]
        
        return np.dot(input,self.weights) + self.biases
    
    def backward(self,input,grad_output):
    
* compute d f / d x = d f / d dense * d dense / d x*
* d dense/ d x = weights transposed*

        grad_input = np.dot(grad_output, self.weights.*  
        
* compute gradient w.r.t. weights and biases*

        grad_weights = np.dot(input.T, grad_output)
        grad_biases = grad_output.mean(axis=0)*input.shape[0]
        
        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape
        
*  perform a stochastic gradient descent step. 
        self.weights = self.weights - self.learning_rate * grad_weights
        self.biases = self.biases - self.learning_rate * grad_biases
        
> 
        return grad_input
```
*  loss function
define softmax nonlinearity on top of our network and compute loss given predicted probabilities.

```
def softmax_crossentropy_with_logits(logits,reference_answers):
*Compute crossentropy from logits[batch,n_classes] and ids of correct answers*
    logits_for_answers = logits[np.arange(len(logits)),reference_answers]
    
    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))
    
    return xentropy

def grad_softmax_crossentropy_with_logits(logits,reference_answers):
*Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers*
    ones_for_answers = np.zeros_like(logits)
    ones_for_answers[np.arange(len(logits)),reference_answers] = 1
    
    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)
    
    return (- ones_for_answers + softmax) / logits.shape[0]
```
* Connect Network
```
network = []
network.append(Dense(X_train.shape[1],100))
network.append(ReLU())
network.append(Dense(100,200))
network.append(ReLU())
network.append(Dense(200,10))
```
* Define network as layer,applied on top of previous one
```
def forward(network, X):
* Compute activations of all network layers by applyingthem sequentially.*
*Return a list of activations for each layer.* 
 
    activations = []
    input = X
* Looping through each layer
    for l in network:
        activations.append(l.forward(input))
*Updating input to last layer output
        input = activations[-1]
    
    assert len(activations) == len(network)
    return activations
```
* Predicting
```
def predict(network,X):
*Compute network predictions. Returning indices of largest Logit probability
    logits = forward(network,X)[-1]
    return logits.argmax(axis=-1)

```
* Train
```
*Train our network on a given batch of X and y.
*run forward to get all layer activations.
*run layer.backward going from last to first layer.
*called backward for all layers, all Dense layers have already made one gradient step.

    
*Get the layer activations
    layer_activations = forward(network,X)
    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]
    logits = layer_activations[-1]
    
*Compute the loss and the initial gradient
    loss = softmax_crossentropy_with_logits(logits,y)
    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)
```
* Backprogation
```
* Propagate gradients through the network
* Reverse propogation as this is backprop
    for layer_index in range(len(network))[::-1]:
        layer = network[layer_index]
        
        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates
        
    return np.mean(loss)
```
* split data into minibatches, feed each such minibatch into the network and update weights.
```
def iterate_minibatches(inputs, targets, batchsize, shuffle=False):
    assert len(inputs) == len(targets)
    if shuffle:
        indices = np.random.permutation(len(inputs))
    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):
        if shuffle:
            excerpt = indices[start_idx:start_idx + batchsize]
        else:
            excerpt = slice(start_idx, start_idx + batchsize)
        yield inputs[excerpt], targets[excerpt]
```
